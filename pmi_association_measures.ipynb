{
 "metadata": {
  "name": "",
  "signature": "sha256:df3c818930b538db3bcc4cc42baf1b4560934e41fbb117583a40592562a3bd1d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>This notebook was put together by [Roman Prokofyev](http://prokofyev.ch) for [eXascale Infolab](http://exascale.info/) research. Source and license info is on [GitHub](https://github.com/dragoon/kilogram/).</i></small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small>\n",
      "This notebook a part of bigger tutorial on fixing grammatical edits.\n",
      "\n",
      "* [Part 1: Extracting edits from StackExchange Data](http://nbviewer.ipython.org/github/dragoon/dataset-utils/blob/master/stackexchange_edit_extraction.ipynb)\n",
      "* **[Part 2: Computing association measures between words](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/pmi_association_measures.ipynb)**\n",
      "* Part 3: Evaluation\n",
      "</small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Prerequisites\n",
      "\n",
      "You will need to install the following python packages to run the notebook:\n",
      "\n",
      "* pip install nltk\n",
      "* pip install pandas\n",
      "* pip install -U https://github.com/dragoon/dataset-utils/zipball/master\n",
      "* pip install -U https://github.com/dragoon/kilogram/zipball/master\n",
      "* pip install -U https://github.com/dragoon/pyutils/zipball/master"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 0: Obtaining n-gram counts\n",
      "\n",
      "We need to download n-gram counts from some sufficiently large corpus.\n",
      "In this example we used **Google Books N-gram** corpus version **20120701**.\n",
      "The dataset is split into multiple parts and can be downloaded from here: http://storage.googleapis.com/books/ngrams/books/datasetsv2.html\n",
      "\n",
      "Tte dataset contains n-gram counts per year, which we don't need here.\n",
      "Therefore we aggregate by year in Hadoop first.\n",
      "We also remove POS-tagged n-gram and lowercase everything.\n",
      "This step significantly reduces the amount of data we need to store, so we can, for example,\n",
      "load all **2grams** into **HBase**.\n",
      "The sizes of ngrams were reduced to the following:\n",
      "\n",
      "| Type   | Original Size (gzipped) | New Size  |\n",
      "| ------ |:-----------------------:| :--------:|\n",
      "| 1grams | 5.3GB | 90MB   |\n",
      "| 2grams | 135GB | 1.9GB  |\n",
      "| 3grams | 1.3TB | 9.5GB  |\n",
      "| 4grams | 294GB | 17.6GB |\n",
      "\n",
      "Additionally we try to aggregate n-grams containing numbers by their conceptual meaning using regular expressions.\n",
      "For example, we distinguish **float numbers**, **percentages**, **date-times**, etc. The exact mapper/reducer files we used are located at https://github.com/dragoon/kilogram/tree/master/mapreduce (Hadoop Python Streaming)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 1: Loading n-gram counts into database\n",
      "\n",
      "Despite significantly reducing sizes of n-gram counts corpus, it is still too large to be uploaded into relational database.\n",
      "Moreover, we don't need any transaction support that will slow down operations.\n",
      "\n",
      "So for our task, we decided to use the following:\n",
      "\n",
      "* **MongoDB** to store **preposition n-grams**, since they are small enough to fit into one machine;\n",
      "* **HBase** to store other arbitrary n-grams."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 2: Retriving n-gram contexts\n",
      "\n",
      "Small recap from the previous notebook to get the desired contexts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from dataset_utils import tokenize_edits\n",
      "edits = tokenize_edits('/home/roman/travel.tsv')\n",
      "PREPS_1GRAM = set(open('/home/roman/ngrams_data/preps.txt').read().split('\\n'))\n",
      "prep_edits = [x for x in edits if x.edit1 in PREPS_1GRAM and x.edit2 in PREPS_1GRAM]\n",
      "\n",
      "context_ngrams = prep_edits[2].context(size=3)  # size=3 is default\n",
      "print context_ngrams"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total edits extracted: 1432\n",
        "{2: [building in, in the], 3: [biggest building in, building in the, in the world]}\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 3: Computing association measures\n",
      "\n",
      "Given the **kilogram** library, computing association measures becomes extremely simple.\n",
      "\n",
      "All we need is to properly configure the database endpoints:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from kilogram import NgramService\n",
      "from kilogram import EditNgram\n",
      "NgramService.configure(PREPS_1GRAM, mongo_host='localhost', hbase_host=(\"diufpc301\", \"9090\"))\n",
      "\n",
      "ngram = context_ngrams[3][0]\n",
      "ngram.pmi_preps"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[((u'biggest', u'building', 'in'), 0.8633729544803259),\n",
        " ((u'biggest', u'building', 'on'), -0.19414591431927875),\n",
        " ((u'biggest', u'building', 'at'), -2.0029574525990768),\n",
        " ((u'biggest', u'building', 'of'), -3.4738843675880844)]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}