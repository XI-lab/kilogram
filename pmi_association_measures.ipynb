{
 "metadata": {
  "name": "",
  "signature": "sha256:609f3580d1f71f442bbae0aed500a141862272165f6d70456bbdfb2a0fce27db"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>This notebook was put together by [Roman Prokofyev](http://prokofyev.ch)@[eXascale Infolab](http://exascale.info/). Source and license info is on [GitHub](https://github.com/dragoon/kilogram/).</i></small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small>\n",
      "This notebook is a part of bigger tutorial on fixing grammatical edits.\n",
      "\n",
      "* [Part 1: Extracting edits from StackExchange Data](http://nbviewer.ipython.org/github/dragoon/dataset-utils/blob/master/stackexchange_edit_extraction.ipynb)\n",
      "* [Part 2: Processing Google Book N-grams dataset](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/process_google_ngrams.ipynb)\n",
      "* **[Part 3: Computing association measures between words](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/pmi_association_measures.ipynb)**\n",
      "* [Part 4: Generic data analysis](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/data_analysis_generic.ipynb)\n",
      "</small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Prerequisites\n",
      "\n",
      "You will need to install the following python packages to run the notebook:\n",
      "\n",
      "* pip install nltk\n",
      "* pip install pandas\n",
      "* pip install -U https://github.com/dragoon/kilogram/zipball/master"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 1: Retriving n-gram contexts\n",
      "\n",
      "Small recap from the previous notebook to get the desired contexts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from kilogram import extract_edits\n",
      "edits = extract_edits('/home/roman/travel.tsv')\n",
      "PREPS_1GRAM = set(open('/home/roman/ngrams_data/preps.txt').read().split('\\n'))\n",
      "prep_edits = [x for x in \n",
      "edits if x.edit1 in PREPS_1GRAM and x.edit2 in PREPS_1GRAM]\n",
      "\n",
      "context_ngrams = prep_edits[2].ngram_context(size=3)  # size=3 is default\n",
      "print context_ngrams"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total edits extracted: 1334\n",
        "{2: [building in, in the], 3: [biggest building in, building in the, in the world]}\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 2: Computing association measures\n",
      "\n",
      "Given the **[kilogram](https://github.com/dragoon/kilogram)** library, computing association measures becomes extremely simple.\n",
      "\n",
      "All we need is to properly configure the endpoints to HBase and MongoDB which store our n-gram data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from kilogram import NgramService\n",
      "from kilogram import EditNgram\n",
      "NgramService.configure(PREPS_1GRAM, mongo_host=('localhost', '27017'), hbase_host=('diufpc301', '9090'))\n",
      "\n",
      "def print_measure(results):\n",
      "    for res in results:\n",
      "        print ' '.join(res[0]), res[1]\n",
      "    print\n",
      "\n",
      "ngram = context_ngrams[3][0]\n",
      "# PMI by default\n",
      "print 'PMI:'\n",
      "print_measure(ngram.association())\n",
      "# Can also use anything implemented in NLTK\n",
      "print 'Student T:'\n",
      "print_measure(ngram.association('student_t'))\n",
      "print 'MI Likelihood:'\n",
      "print_measure(ngram.association('mi_like'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "PMI:\n",
        "biggest building in 0.86337295448\n",
        "biggest building on -0.194145914319\n",
        "biggest building at -2.0029574526\n",
        "biggest building of -3.47388436759\n",
        "\n",
        "Student T:\n",
        "biggest building on -16.6575710442\n",
        "biggest building in -21.945374765\n",
        "biggest building at -25.9761580745\n",
        "biggest building of -130.521765404\n",
        "\n",
        "MI Likelihood:\n",
        "biggest building in 2.77298228106e-15\n",
        "biggest building on 2.35666190597e-17\n",
        "biggest building of 1.02821534354e-18\n",
        "biggest building at 2.64008318578e-19\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    }
   ],
   "metadata": {}
  }
 ]
}