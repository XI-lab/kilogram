{
 "metadata": {
  "name": "",
  "signature": "sha256:8b49e1bd1f953b7d7550a2f04e1c4f36544062f6a68c9d22e6e019d3d5a51e78"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>This notebook was put together by [Roman Prokofyev](http://prokofyev.ch) for [eXascale Infolab](http://exascale.info/) research. Source and license info is on [GitHub](https://github.com/dragoon/kilogram/).</i></small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small>\n",
      "This notebook is a part of bigger tutorial on fixing grammatical edits.\n",
      "\n",
      "* [Part 1: Extracting edits from StackExchange Data](http://nbviewer.ipython.org/github/dragoon/dataset-utils/blob/master/stackexchange_edit_extraction.ipynb)\n",
      "* **[Part 2: Processing Google Book N-grams dataset](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/process_google_ngrams.ipynb)**\n",
      "* [Part 3: Computing association measures between words](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/pmi_association_measures.ipynb)\n",
      "* [Part 4: Generic data analysis](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/data_analysis_generic.ipynb)\n",
      "</small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Prerequisites\n",
      "\n",
      "* Hadoop cluster with **Cloudera Hadoop Distribution** (CDH5): HDFS, YARN, PIG, HBASE"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 0: Downloading n-gram counts\n",
      "\n",
      "We need to download n-gram counts from some sufficiently large corpus.\n",
      "In this example we used **Google Books N-gram** corpus version **20120701**.\n",
      "The dataset is split into multiple parts and can be downloaded from here: http://storage.googleapis.com/books/ngrams/books/datasetsv2.html"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 1: Aggregating counts and identifying entities\n",
      "Tte dataset contains n-gram counts per year, which we don't need here.\n",
      "Therefore we aggregate by year in **Hadoop** first.\n",
      "We also remove POS-tagged n-grams and lowercase everything.\n",
      "This step significantly reduces the amount of data we need to store, so we can, for example,\n",
      "load all **2grams** into **HBase**.\n",
      "\n",
      "The following table shows the sizes of ngrams at different processing stages:\n",
      "\n",
      "| Type   | Original Size (gzipped) | Year-aggregated | Final size |\n",
      "| ------ |:-----------------------:| :--------:| :--------: |\n",
      "| 1grams | 5.3GB | 342MB   | 90MB   |\n",
      "| 2grams | 135GB | 13.4GB  | 1.9GB  |\n",
      "| 3grams | 1.3TB | 190.6GB | 9.5GB  |\n",
      "| 4grams | 294GB | 50.9GB  | 17.6GB |\n",
      "| 5grams | XXXGB | 51.1GB  | 12GB   |\n",
      "\n",
      "\n",
      "At the final processing step, we try to aggregate n-grams containing various entities by their conceptual type using regular expressions and dictionaries.\n",
      "For example, we distinguish **numeric entities**, **person names**, **cities (geo-entities)** and **DBPedia entities** (see below).\n",
      "We will show how to use this information in the next part of this tutotial.\n",
      "\n",
      "Internally **numeric entities** have 5 types which are identified by *regular expressions*:\n",
      "* Datetimes (2 different types)\n",
      "* Percentages\n",
      "* Area or Volume metrics\n",
      "* Generic numbers\n",
      "\n",
      "**Person names** are identified by a dictionary lookup in the **names** corpus of the **NLTK** library.\n",
      "For each match we generate new n-gram with original string replaced by the \"PERSON\" token.\n",
      "\n",
      "**Cities** are also identified by a dictionary lookup. We use **Geonames 15000** as our dictionary. Since it is very small (~2Mb), we ship it with the **kilogram** library. For each match we generate new n-gram with original string replaced by the \"CITY\" token.\n",
      "\n",
      "**DBPedia** entities are a bit more complicated. On one side, there are many interesting properties inside DBPedia that we can leverage on. On the other side, it's **generally impossible to match** arbitrary DBPedia entity inside an n-gram due to lack of context. It would be only possible if we had a list of unambiguous entities that always match their string representations (but we don't have it).\n",
      "However, we found a way to remove incorrect entity linkings(i.e., entities that often match generic phrases) by assuming normal distribution of counts for DBPedia types and filtering out outliers.\n",
      "\n",
      "This method is covered in a separate tutorial:\n",
      "[Linking DBPedia Entities in N-gram corpus](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/notebooks/dbpedia_ner.ipynb)\n",
      "*****"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That was the teoretical part. Coming to the n-gram processing itself.\n",
      "\n",
      "First clone the **[kilogram](https://github.com/dragoon/kilogram)** library:\n",
      "\n",
      "    git clone https://github.com/dragoon/kilogram.git\n",
      "    cd kilogram/mapreduce\n",
      "\n",
      "Install **NLTK** and the **names dataset** on every machine using *parallel-ssh*:\n",
      "\n",
      "    parallel-ssh -h hosts.txt -l root 'pip install nltk'\n",
      "    parallel-ssh -h hosts.txt -l root 'python -m nltk.downloader -d /usr/local/lib/nltk_data names'\n",
      "*Hosts.txt* file should contain a list of hosts in your cluster, one per line.\n",
      "\n",
      "Download required **DBPedia** datasets:\n",
      "\n",
      "    wget http://downloads.dbpedia.org/3.9/en/labels_en.nt.bz2\n",
      "    wget http://downloads.dbpedia.org/3.9/en/redirects_transitive_en.nt.bz2\n",
      "    wget http://downloads.dbpedia.org/3.9/en/instance_types_en.nt.bz2\n",
      "\n",
      "Run the streaming MapReduce job:\n",
      "\n",
      "    ./generic_job.sh [-r NUMBER_OF_REDUCERS] $RAW_NGRAMS_INPUT_DIR $NGRAM_OUT_DIR\n",
      "\n",
      "**Grab a cup of tea!**\n",
      "\n",
      "After the job is finished, we will have a joined ~45GB dataset containing all n-grams in one directory.\n",
      "In the next step, we will extract only the n-grams we are interested in."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 1: Filtering preposition n-gram counts\n",
      "\n",
      "To efficiently compute association measures between words, we need efficient access to every n-gram count up to 3grams.\n",
      "Unfortunately, the new n-gram count sizes are still to large to be put in any **local key-value store**.\n",
      "\n",
      "Another issue with storing n-grams counts as simple key-values is high inefficiency for our **grammatical correction task**.\n",
      "For instance, to correct a preposition in a sentence, we will need to retrieve n-gram counts for all possible prepositions in consideration.\n",
      "\n",
      "Using simple key-value counts, we will have to make one request per preposition per n-gram.\n",
      "However, we know in advance which prepositions we want to consider, so we can aggregate all preposition counts into one value and use special n-gram as a key.\n",
      "We will call such n-grams **preposition n-grams**.\n",
      "\n",
      "Assuming that our current directory is something like ``~/kilogram/mapreduce``, we can filter preposition n-grams using the following scripts:\n",
      "\n",
      "    ./filter/filter_job.sh -n 2 -f ../extra/preps.txt [-r NUMBER_OF_REDUCERS] $NGRAM_OUT_DIR $2GRAMS_PREPS_OUT_DIR\n",
      "    ./filter/filter_job.sh -n 3 -f ../extra/preps.txt [-r NUMBER_OF_REDUCERS] $NGRAM_OUT_DIR $3GRAMS_PREPS_OUT_DIR \n",
      "    \n",
      "This way we will filter only 2grams and 3grams that contain prepositions, and their sizes should be approximately XX and XX respectively. Now we can continue and load them to a local database.\n",
      "\n",
      "To compute association measures for 3grams, we would also require access to arbitrary 2gram counts. We will show how to extract and load them to a storage in the next section."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 2: Loading n-gram counts into database\n",
      "\n",
      "Given what we descussed in the previous section, we decided to use the following solution:\n",
      "\n",
      "* **MongoDB** to store **preposition n-grams**, since they are small enough to fit into one machine;\n",
      "* **HBase** to store other arbitrary n-grams.\n",
      "\n",
      "To upload **preposition n-grams** to MongoDB in the right format, we use scripts from the kilogram library. It needs to be installed via pip as well to run the scripts:\n",
      "\n",
      "    hdfs dfs -cat $2GRAMS_PREPS_OUT_DIR/* > preps_2grams\n",
      "    cat preps_2grams | convert_to_mongo.py --sub preps.txt | sort -k1,1 -t $'\\t' > preps_2grams_mongo\n",
      "    insert_mongo.py -d 2grams --subs preps_2grams_mongo\n",
      "    \n",
      "    hdfs dfs -cat $3GRAMS_PREPS_OUT_DIR/* > preps_3grams\n",
      "    cat preps_3grams | convert_to_mongo.py --sub preps.txt | sort -k1,1 -t $'\\t' > preps_3grams_mongo\n",
      "    insert_mongo.py -d 3grams --subs preps_3grams_mongo\n",
      "    \n",
      "File ``preps.txt`` is our preposition set, which counts 49 prepositions in total.\n",
      "\n",
      "Next, we upload 2grams to **HBase** table names **ngrams2** using [Pig](http://pig.apache.org/): \n",
      "    \n",
      "    pig -p table=ngrams2 path=$NGRAM_OUT_DIR n=2 ../extra/hbase_upload.pig\n",
      "\n",
      "**Grab another cup of something, this will take a while.**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next part of this notebook describes how to calculate association measures between words in order to fix grammar."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}