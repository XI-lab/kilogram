{
 "metadata": {
  "name": "",
  "signature": "sha256:6149adde85697bd08bffab7f87fb25c1c4deecd87e58d21846441864243e62dc"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>This notebook was put together by [Roman Prokofyev](http://prokofyev.ch) for [eXascale Infolab](http://exascale.info/) research. Source and license info is on [GitHub](https://github.com/dragoon/kilogram/).</i></small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small>\n",
      "This notebook is a part of bigger tutorial on fixing grammatical edits.\n",
      "\n",
      "* [Part 1: Extracting edits from StackExchange Data](http://nbviewer.ipython.org/github/dragoon/dataset-utils/blob/master/stackexchange_edit_extraction.ipynb)\n",
      "* **[Part 2: Processing Google Book N-grams dataset](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/process_google_ngrams.ipynb)**\n",
      "* [Part 3: Computing association measures between words](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/pmi_association_measures.ipynb)\n",
      "* [Part 4: Generic data analysis](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/data_analysis_generic.ipynb)\n",
      "</small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 0: Downloading n-gram counts\n",
      "\n",
      "We need to download n-gram counts from some sufficiently large corpus.\n",
      "In this example we used **Google Books N-gram** corpus version **20120701**.\n",
      "The dataset is split into multiple parts and can be downloaded from here: http://storage.googleapis.com/books/ngrams/books/datasetsv2.html"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 1: \n",
      "Tte dataset contains n-gram counts per year, which we don't need here.\n",
      "Therefore we aggregate by year in **Hadoop** first.\n",
      "We also remove POS-tagged n-grams and lowercase everything.\n",
      "This step significantly reduces the amount of data we need to store, so we can, for example,\n",
      "load all **2grams** into **HBase**.\n",
      "\n",
      "The following table shows the sizes of ngrams at different processing stages:\n",
      "\n",
      "| Type   | Original Size (gzipped) | Year-aggregated | Final size |\n",
      "| ------ |:-----------------------:| :--------:| :--------: |\n",
      "| 1grams | 5.3GB | 342MB   | 90MB   |\n",
      "| 2grams | 135GB | 13.4GB  | 1.9GB  |\n",
      "| 3grams | 1.3TB | 190.6GB | 9.5GB  |\n",
      "| 4grams | 294GB | 50.9GB  | 17.6GB |\n",
      "| 5grams | XXXGB | 51.1GB  | 12GB   |\n",
      "\n",
      "\n",
      "At the final processing step, we try to aggregate n-grams containing various entities by their conceptual type using regular expressions and dictionaries.\n",
      "For example, we distinguish **numeric entities**, **person names**, **cities** and **DBPedia entities types**.\n",
      "We will show how to use this information in the next part of this tutotial.\n",
      "\n",
      "Internally **numeric entities** have 5 types which are identified by *regular expressions*:\n",
      "* Datetimes (2 different types)\n",
      "* Percentages\n",
      "* Area or Volume metrics\n",
      "* Generic numbers\n",
      "\n",
      "**Person names** are identified by a dictionary lookup in the **names** corpus of the **NLTK** library.\n",
      "For each match we generate new n-gram with original string replaced by the \"PERSON\" token.\n",
      "\n",
      "**Cities** are also identified by a dictionary lookup. We use **Geonames 15000** as our dictionary. Since it is very small (~2Mb), we ship it with the **kilogram** library. For each match we generate new n-gram with original string replaced by the \"CITY\" token.\n",
      "\n",
      "**DBPedia** entities are a bit more complicated, since there are many interesting properties that we can leverage on.\n",
      "In this tutorial, we perform entity recognition based on the longest string match with the **label** and **wikiRedirects** properties. If more than one entity matches original string, we generate new n-grams **for each match** with original string replaced by canonical URL of entity.\n",
      "*****"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That was the teoretical part. Coming to the n-gram processing itself.\n",
      "\n",
      "First clone the **kilogram** library:\n",
      "\n",
      "    git clone https://github.com/dragoon/kilogram.git\n",
      "    cd kilogram/mapreduce\n",
      "\n",
      "Install **NLTK** and the **names dataset** on every machine using *parallel-ssh*:\n",
      "\n",
      "    parallel-ssh -h hosts.txt -l root 'pip install nltk'\n",
      "    parallel-ssh -h hosts.txt -l root 'python -m nltk.downloader -d /usr/local/lib/nltk_data names'\n",
      "*Hosts.txt* file should contain a list of hosts in your cluster, one per line.\n",
      "\n",
      "Download required **DBPedia** datasets:\n",
      "\n",
      "    wget http://downloads.dbpedia.org/3.9/en/labels_en.nt.bz2\n",
      "    wget http://downloads.dbpedia.org/3.9/en/redirects_transitive_en.nt.bz2\n",
      "    wget http://downloads.dbpedia.org/3.9/en/instance_types_en.nt.bz2\n",
      "\n",
      "Run the streaming MapReduce job (assumes you have **Cloudera Hadoop Distribution**):\n",
      "\n",
      "    ./generic_job.sh INPUT_DIR OUTPUT_DIR [NUMBER_OF_REDUCERS]\n",
      "\n",
      "Grab a cup of tea!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 1: Filtering preposition n-gram counts\n",
      "\n",
      "To efficiently compute association measures between words, we need efficient access to every n-gram count up to 3grams.\n",
      "Unfortunately, the new n-gram count sizes are still to large to be put in any **local key-value store**.\n",
      "\n",
      "Another issue with storing n-grams counts as simple key-values is high inefficiency for our **grammatical correction task**.\n",
      "For instance, to correct a preposition in a sentence, we will need to retrieve n-gram counts for all possible prepositions in consideration.\n",
      "\n",
      "Using simple key-value counts, we will have to make one request per preposition per n-gram.\n",
      "However, we know in advance which prepositions we want to consider, so we can aggregate all preposition counts into one value and use special n-gram as a key.\n",
      "We will call such n-grams **preposition n-grams**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 2: Loading n-gram counts into database\n",
      "\n",
      "Given what we descussed in the previous section, we decided to use the following solution:\n",
      "\n",
      "* **MongoDB** to store **preposition n-grams**, since they are small enough to fit into one machine;\n",
      "* **HBase** to store other arbitrary n-grams.\n",
      "\n",
      "To generate proper **preposition n-grams** and upload them to MongoDB, we use scripts from the **[kilogram](https://github.com/dragoon/kilogram)** library. As our preposition set, we use the following: https://github.com/dragoon/kilogram/blob/master/extra/preps.txt.\n",
      "\n",
      "Here is an example for 3grams:\n",
      "\n",
      "    cat google_3grams | convert_to_mongo.py --sub preps.txt | sort -k1,1 -t $'\\t' > 3grams_mongo\n",
      "    insert_mongo.py -d 3grams --subs 3grams_mongo\n",
      "    \n",
      "To upload ngrams to **HBase**, we will use the following Pig script: https://github.com/dragoon/kilogram/blob/master/extra/hbase_upload.pig"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next part of this notebook describes how to calculate association measures between words in order to fix grammar."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}