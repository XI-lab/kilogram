{
 "metadata": {
  "name": "",
  "signature": "sha256:267da754d20bdb22c24296993a42777212d34a21bdcd3aa400261a729a07247c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>This notebook was put together by [Roman Prokofyev](http://prokofyev.ch) for [eXascale Infolab](http://exascale.info/) research. Source and license info is on [GitHub](https://github.com/dragoon/kilogram/).</i></small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small>\n",
      "This notebook is a part of bigger tutorial on fixing grammatical edits.\n",
      "\n",
      "* [Part 1: Extracting edits from StackExchange Data](http://nbviewer.ipython.org/github/dragoon/dataset-utils/blob/master/stackexchange_edit_extraction.ipynb)\n",
      "* **[Part 2: Processing Google Book N-grams dataset](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/process_google_ngrams.ipynb)**\n",
      "* [Part 3: Computing association measures between words](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/pmi_association_measures.ipynb)\n",
      "* [Part 4: Generic data analysis](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/data_analysis_generic.ipynb)\n",
      "</small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 0: Obtaining n-gram counts\n",
      "\n",
      "We need to download n-gram counts from some sufficiently large corpus.\n",
      "In this example we used **Google Books N-gram** corpus version **20120701**.\n",
      "The dataset is split into multiple parts and can be downloaded from here: http://storage.googleapis.com/books/ngrams/books/datasetsv2.html\n",
      "\n",
      "Tte dataset contains n-gram counts per year, which we don't need here.\n",
      "Therefore we aggregate by year in Hadoop first.\n",
      "We also remove POS-tagged n-gram and lowercase everything.\n",
      "This step significantly reduces the amount of data we need to store, so we can, for example,\n",
      "load all **2grams** into **HBase**.\n",
      "The sizes of ngrams were reduced to the following:\n",
      "\n",
      "| Type   | Original Size (gzipped) | Year-aggregated | Final size |\n",
      "| ------ |:-----------------------:| :--------:| :--------: |\n",
      "| 1grams | 5.3GB | 342MB   | 90MB   |\n",
      "| 2grams | 135GB | 13.4GB  | 1.9GB  |\n",
      "| 3grams | 1.3TB | 190.6GB | 9.5GB  |\n",
      "| 4grams | 294GB | 50.9GB  | 17.6GB |\n",
      "| 5grams | XXXGB | 51.1GB  | 12GB   |\n",
      "\n",
      "Additionally we try to aggregate n-grams containing various entities by their conceptual type using regular expressions and dictionaries.\n",
      "For example, we distinguish **float numbers**, **percentages**, **date-times**, **person names** and other entities.\n",
      "We will show how to use this information in the next part of this tutotial.\n",
      "The exact mapper/reducer scripts we use can be found on https://github.com/dragoon/kilogram/tree/master/mapreduce (Hadoop Python Streaming)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 1: Filtering preposition n-gram counts\n",
      "\n",
      "To efficiently compute association measures between words, we need efficient access to every n-gram count up to 3grams.\n",
      "Unfortunately, the new n-gram count sizes are still to large to be put in any **local key-value store**.\n",
      "\n",
      "Another issue with storing n-grams counts as simple key-values is high inefficiency for our **grammatical correction task**.\n",
      "For instance, to correct a preposition in a sentence, we will need to retrieve n-gram counts for all possible prepositions in consideration.\n",
      "\n",
      "Using simple key-value counts, we will have to make one request per preposition per n-gram.\n",
      "However, we know in advance which prepositions we want to consider, so we can aggregate all preposition counts into one value and use special n-gram as a key.\n",
      "We will call such n-grams **preposition n-grams**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 2: Loading n-gram counts into database\n",
      "\n",
      "Given what we descussed in the previous section, we decided to use the following solution:\n",
      "\n",
      "* **MongoDB** to store **preposition n-grams**, since they are small enough to fit into one machine;\n",
      "* **HBase** to store other arbitrary n-grams.\n",
      "\n",
      "To generate proper **preposition n-grams** and upload them to MongoDB, we use scripts from the **[kilogram](https://github.com/dragoon/kilogram)** library. As our preposition set, we use the following: https://github.com/dragoon/kilogram/blob/master/extra/preps.txt.\n",
      "\n",
      "Here is an example for 3grams:\n",
      "\n",
      "    cat google_3grams | convert_to_mongo.py --sub preps.txt | sort -k1,1 -t $'\\t' > 3grams_mongo\n",
      "    insert_mongo.py -d 3grams --subs 3grams_mongo\n",
      "    \n",
      "To upload ngrams to **HBase**, we will use the following Pig script: https://github.com/dragoon/kilogram/blob/master/extra/hbase_upload.pig"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}