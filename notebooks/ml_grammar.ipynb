{
 "metadata": {
  "name": "",
  "signature": "sha256:95d1f8760d06b05d8b3483808e8befa8ce0f29b2603e1961dbfb52a9241c6d0a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>This notebook was put together by [Roman Prokofyev](http://prokofyev.ch)@[eXascale Infolab](http://exascale.info/). Source and license info is on [GitHub](https://github.com/dragoon/kilogram/).</i></small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small>\n",
      "This notebook is a part of bigger tutorial on fixing grammatical edits.\n",
      "\n",
      "* [Part 1: Extracting edits from StackExchange Data](http://nbviewer.ipython.org/github/dragoon/dataset-utils/blob/master/notebooks/stackexchange_edit_extraction.ipynb)\n",
      "* [Part 2: Processing Google Book N-grams dataset](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/notebooks/process_google_ngrams.ipynb)\n",
      "* [Part 3: Computing association measures between words](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/notebooks/pmi_association_measures.ipynb)\n",
      "* [Part 4: Generic data analysis](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/notebooks/data_analysis_generic.ipynb)\n",
      "* **[Part 5: Machine learning to fix grammar](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/notebooks/ml_grammar.ipynb)**\n",
      "</small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Prerequisites\n",
      "\n",
      "You will need to install the following python packages to run the notebook:\n",
      "\n",
      "* pip install scikit-learn\n",
      "\n",
      "Download [Stanford POS Tagger](http://nlp.stanford.edu/software/tagger.shtml#Download), for example current latest version is 3.5: http://nlp.stanford.edu/software/stanford-postagger-2014-10-26.zip. It requires **Java 8** to run.\n",
      "\n",
      "Start POS tagger as a network service:\n",
      "    \n",
      "    java -mx300m -cp stanford-postagger.jar edu.stanford.nlp.tagger.maxent.MaxentTaggerServer -model models/english-bidirectional-distsim.tagger -port 2020"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division\n",
      "import pandas as pd\n",
      "from sklearn.ensemble import ExtraTreesClassifier\n",
      "from sklearn.cross_validation import StratifiedShuffleSplit, ShuffleSplit\n",
      "from kilogram import NgramService\n",
      "from kilogram import EditNgram\n",
      "\n",
      "PREPS = set(open('../extra/preps.txt').read().split('\\n'))\n",
      "PREPS_SORTED = sorted(PREPS)\n",
      "NgramService.configure(PREPS_SORTED, mongo_host=('localhost', '27017'), hbase_host=('diufpc301', '9090'))\n",
      "ALLOWED_TYPES={1,2,3}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from kilogram import extract_edits\n",
      "prep_edits = extract_edits('/home/roman/fce_edits.tsv', substitutions=PREPS)\n",
      "total_error = len([1 for x in prep_edits if x.is_error])\n",
      "print 'Total errors:', total_error"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total edits extracted: 60097\n",
        "Total errors: 3046\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from kilogram.edit import EditCollection\n",
      "collection = EditCollection(prep_edits)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# QUICK HACK CONLL parser\n",
      "import re\n",
      "from kilogram.edit import Edit\n",
      "EDIT_RE = re.compile(r'\\(\\w+\\*/(\\w+)\\)')\n",
      "from kilogram.lang.tokenize import default_tokenize_func\n",
      "test_col = []\n",
      "conll_data = open('/home/roman/english_learners/data/conll-test.txt').read().splitlines()\n",
      "for line in conll_data:\n",
      "    tokens = default_tokenize_func(line, set('!\",:;<=>?[]{}.?'))\n",
      "    context = EDIT_RE.sub(r'\\1', ' '.join(tokens))\n",
      "    for i, token in enumerate(tokens):\n",
      "        if EDIT_RE.match(token):\n",
      "            edit1, edit2 = token[1:-1].split('*/')\n",
      "            test_col.append(Edit(edit1, edit2, context, context, (i, i+1), (i, i+1)))\n",
      "        elif token in PREPS:\n",
      "            test_col.append(Edit(token, token, context, context, (i, i+1), (i, i+1)))\n",
      "print 'Test collection size:', len(test_col)\n",
      "print 'Total errors:', len([1 for x in test_col if x.is_error])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test collection size: 2945\n",
        "Total errors: 152\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import kilogram\n",
      "kilogram.DEBUG = False\n",
      "# for Stanford tagger\n",
      "kilogram.edit.ST_PORT = 2020\n",
      "res = []\n",
      "for i in xrange(1):\n",
      "    train_data, train_labels, feature_names = collection.balance_features(PREPS_SORTED, class1_k=1, class0_k=3046./60097)\n",
      "    clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
      "    print 'Fitting...'\n",
      "    clf.fit(train_data, train_labels)\n",
      "    print sorted(zip(list(clf.feature_importances_), feature_names), key=lambda x: x[0], reverse=True)\n",
      "    print 'Validating...'\n",
      "    score = collection.test_validation(PREPS_SORTED, clf, test_col)\n",
      "    print score\n",
      "    res.append(score)\n",
      "print np.mean([x['f1'] for x in res]), np.std([x['f1'] for x in res])\n",
      "print np.mean([x['precision'] for x in res]), np.std([x['precision'] for x in res])\n",
      "print np.mean([x['recall'] for x in res]), np.std([x['recall'] for x in res])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Balancing errors\n",
        "Generating features from raw data"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Started data loading: 14:55:24\n",
        "Finish data loading: 14:57:58"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Converting to numpy arrays\n",
        "(290913, 75)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(290913,)\n",
        "('1st class', 5937)\n",
        "('2nd class', 284976)\n",
        "Fitting...\n",
        "[(0.20798748843339482, 'conf_matrix_score'), (0.10977880317463766, 'top_prep_count_3gram'), (0.069689370434173323, 'avg_rank_3gram'), (0.061832706707920633, 'avg_pmi_3gram'), (0.052726091313261607, 'zero_ngram_rank'), (0.051340063841547677, 'avg_rank_position_1'), (0.049254518653564096, 'avg_rank_position_-1'), (0.047459487157366712, 'avg_rank_position_0'), (0.04620038435426925, 'avg_pmi_2gram'), (0.040430869679560485, 'avg_rank_2gram'), (0.034567433796767626, 'to'), (0.030340108692495354, 'top_prep_count_2gram'), (0.020573668312620041, 'in'), (0.013576677316233734, 'NNnext'), (0.013288989972568711, 'OTHERprev'), (0.012582908332913883, 'of'), (0.012125360098724988, 'VBnext'), (0.011865129495164127, 'OTHERnext'), (0.011271982613862802, 'DTprev'), (0.0095776108149224614, 'at'), (0.0095157192659900709, 'on'), (0.0094676374903120836, 'VBprev'), (0.0086476766944613703, 'NNprev'), (0.0083743574084220746, 'for'), (0.0073197328433857144, 'JJnext'), (0.007076145399618791, 'RBnext'), (0.0050967674817830773, 'DTnext'), (0.0040958308307860094, 'JJprev'), (0.0040220103702215924, 'about'), (0.0034745979054994669, 'but'), (0.0034544052949698595, 'from'), (0.0025905732027250213, 'by'), (0.0022682067801574669, 'RBprev'), (0.0021360741779477528, 'during'), (0.0019854697757812879, 'into'), (0.001566511079054024, 'with'), (0.0012039544226070931, 'until'), (0.0011911809309979463, 'after'), (0.00093133720769761471, 'across'), (0.00091191908101887094, 'through'), (0.0007657426490566754, 'over'), (0.0007369749060832161, 'before'), (0.00065276754776406561, 'than'), (0.00061756120424786415, 'since'), (0.00052610146306399889, 'beyond'), (0.00050897767370425772, 'upon'), (0.00050690905087802276, 'off'), (0.00046873872932459508, 'onto'), (0.00044180912059886958, 'between'), (0.00039170694025098191, 'behind'), (0.00031634383347666926, 'inside'), (0.0002953728719462355, 'outside'), (0.00026476186144991885, 'around'), (0.00023576822109776452, 'among'), (0.00022204172405094067, 'against'), (0.00017120421683512797, 'above'), (0.00016202124301317164, 'towards'), (0.00013993212762593678, 'beside'), (0.00013933123417962515, 'under'), (0.000119697077809817, 'except'), (0.00011624393319307681, 'along'), (8.8164061017333999e-05, 'amongst'), (4.544847537468281e-05, 'opposite'), (4.1183421368084625e-05, 'toward'), (3.9309066076331533e-05, 'absent'), (3.750543260656916e-05, 'alongside'), (3.5536024691051233e-05, 'amid'), (3.0945924003499808e-05, 'below'), (2.5022384763469612e-05, 'despite'), (2.4785957240537667e-05, 'underneath'), (2.3227908967827743e-05, 'besides'), (9.1028748326450942e-06, 'beneath'), (0.0, 'has_zero_ngram_2_or_3'), (0.0, 'PRprev'), (0.0, 'PRnext')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Validating...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Started data loading: 15:00:13\n",
        "Finish data loading: 15:01:33"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Total errors: 152\n",
        "{'f1': 0.20187793427230044, 'false': 231, 'min_split': 2, 'true_err': 43, 'precision': 0.15693430656934307, 'skips': 738, 'false_err': 33, 'true': 1968, 'recall': 0.28289473684210525, 'depth': None, 'skips_err': 68, 'accuracy': 0.8949522510231923}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.201877934272 0.0\n",
        "0.156934306569 0.0\n",
        "0.282894736842 0.0\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Plotting decision tree (example)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image #needed to render in notebook\n",
      "import StringIO, pydot  #needed to convert dot format to png\n",
      "clf = DecisionTreeClassifier(max_depth=5, min_samples_leaf=10)\n",
      "train_col, train_labels = balance(data, labels)\n",
      "clf.fit(train_col, train_labels)\n",
      "dot_data = StringIO.StringIO()\n",
      "export_graphviz(clf, out_file=dot_data, feature_names=feature_names) \n",
      "graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
      "Image(graph.create_png())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "context_ngrams = [x for x in test_col if x.is_error][0].ngram_context(3)\n",
      "df_list_substs = []\n",
      "df_list_substs_enhanced = []\n",
      "positions_set = set()\n",
      "# RANK, PMI_SCORE\n",
      "DEFAULT_SCORE = (50, -10)\n",
      "# TODO: filter on ALLOWED_TYPES\n",
      "for ngram_type, ngrams in reversed(context_ngrams.items()):\n",
      "    for ngram_pos, ngram in enumerate(ngrams):\n",
      "        subst_pos = ngram_type - 1 - ngram_pos\n",
      "        if ngram:\n",
      "            score_dict = dict((x[0][subst_pos], (i, x[1])) for i, x in enumerate(ngram.association()))\n",
      "        else:\n",
      "            score_dict = {}\n",
      "        new_pos = 0\n",
      "        if ngram_pos == 0:\n",
      "            new_pos = -1\n",
      "        elif ngram_pos == (ngram_type - 1):\n",
      "            new_pos = 1\n",
      "        for subst in PREPS_SORTED:\n",
      "            df_list_substs.append([subst, score_dict.get(subst, DEFAULT_SCORE)[1],\n",
      "                                   score_dict.get(subst, DEFAULT_SCORE)[0],\n",
      "                                   ngram_type, new_pos])\n",
      "            if new_pos in positions_set:\n",
      "                df_list_substs_enhanced.append([subst, DEFAULT_SCORE[1], DEFAULT_SCORE[0],\n",
      "                                                ngram_type, new_pos])\n",
      "            else:\n",
      "                df_list_substs_enhanced.append([subst, score_dict.get(subst, DEFAULT_SCORE)[1],\n",
      "                   score_dict.get(subst, DEFAULT_SCORE)[0], ngram_type, new_pos])\n",
      "        positions_set.add(new_pos)\n",
      "df_substs = pd.DataFrame(df_list_substs, columns=['substitution', 'score', 'rank', 'type', 'norm_position'])\n",
      "df_substs_enhanced = pd.DataFrame(df_list_substs_enhanced, columns=['substitution', 'score', 'rank', 'type', 'norm_position'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type_group = df_substs_enhanced[(df_substs_enhanced.substitution=='at')].groupby(['substitution', 'type'])\n",
      "type_group.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th>score</th>\n",
        "      <th>rank</th>\n",
        "      <th>norm_position</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>substitution</th>\n",
        "      <th>type</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th rowspan=\"2\" valign=\"top\">at</th>\n",
        "      <th>2</th>\n",
        "      <td>-10.000000</td>\n",
        "      <td> 50.000000</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> -2.267814</td>\n",
        "      <td> 22.333333</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "                       score       rank  norm_position\n",
        "substitution type                                     \n",
        "at           2    -10.000000  50.000000              0\n",
        "             3     -2.267814  22.333333              0"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_substs_enhanced[(df_substs_enhanced.substitution=='at')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>substitution</th>\n",
        "      <th>score</th>\n",
        "      <th>rank</th>\n",
        "      <th>type</th>\n",
        "      <th>norm_position</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>12 </th>\n",
        "      <td> at</td>\n",
        "      <td> -0.695445</td>\n",
        "      <td> 17</td>\n",
        "      <td> 3</td>\n",
        "      <td>-1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>61 </th>\n",
        "      <td> at</td>\n",
        "      <td>-10.000000</td>\n",
        "      <td> 50</td>\n",
        "      <td> 3</td>\n",
        "      <td> 0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>110</th>\n",
        "      <td> at</td>\n",
        "      <td>  3.892003</td>\n",
        "      <td>  0</td>\n",
        "      <td> 3</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>159</th>\n",
        "      <td> at</td>\n",
        "      <td>-10.000000</td>\n",
        "      <td> 50</td>\n",
        "      <td> 2</td>\n",
        "      <td>-1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>208</th>\n",
        "      <td> at</td>\n",
        "      <td>-10.000000</td>\n",
        "      <td> 50</td>\n",
        "      <td> 2</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "    substitution      score  rank  type  norm_position\n",
        "12            at  -0.695445    17     3             -1\n",
        "61            at -10.000000    50     3              0\n",
        "110           at   3.892003     0     3              1\n",
        "159           at -10.000000    50     2             -1\n",
        "208           at -10.000000    50     2              1"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = test_col[0].ngram_context(3)[2][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a.ngram = ('TIME1', 'in')\n",
      "a._association_dict = {}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a.association()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "[(('TIME1', 'until'), -3.0774409745490203),\n",
        " (('TIME1', 'to'), -4.355247055121687),\n",
        " (('TIME1', 'in'), -4.552643557058502),\n",
        " (('TIME1', 'on'), -4.601226889675182),\n",
        " (('TIME1', 'at'), -5.473221249068018),\n",
        " (('TIME1', 'for'), -5.9084670751569135),\n",
        " (('TIME1', 'before'), -6.175736406019048),\n",
        " (('TIME1', 'but'), -6.886017481366906),\n",
        " (('TIME1', 'after'), -7.270698518707498),\n",
        " (('TIME1', 'between'), -7.401036839336669),\n",
        " (('TIME1', 'by'), -7.923704106803363),\n",
        " (('TIME1', 'during'), -8.408474776734451),\n",
        " (('TIME1', 'from'), -8.589846770511855),\n",
        " (('TIME1', 'into'), -9.511438547990423),\n",
        " (('TIME1', 'through'), -9.797133743859774),\n",
        " (('TIME1', 'of'), -10.39420477290441),\n",
        " (('TIME1', 'over'), -12.57311745670517),\n",
        " (('TIME1', 'about'), -13.180848724334567)]"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}