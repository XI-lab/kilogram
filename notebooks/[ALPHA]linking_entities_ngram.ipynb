{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small><i>This notebook was put together by [Roman Prokofyev](http://prokofyev.ch)@[eXascale Infolab](http://exascale.info/). Source and license info is on [GitHub](https://github.com/dragoon/kilogram/).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "* Pandas: ``pip install pandas``\n",
    "* Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking DBPedia entity types in N-gram corpus\n",
    "In this sub-tutorial, we will link DBPedia entities (and subsequently types) in an N-gram corpus. In our case, we used n-grams generated out of **Wikipedia pages**, to compare if our linking is better than original.\n",
    "\n",
    "Clone **kilogram** library if you haven't done it yet:\n",
    "\n",
    "    git clone https://github.com/dragoon/kilogram.git\n",
    "    cd kilogram/mapreduce\n",
    "    \n",
    "Download required **DBPedia** datasets:\n",
    "    \n",
    "    wget http://downloads.dbpedia.org/2015-04/dbpedia_2015-04.owl.bz2\n",
    "    wget -O redirects_transitive_en.nt.bz2 http://downloads.dbpedia.org/2015-04/core-i18n/en/transitive-redirects_en.nt.bz2\n",
    "    wget -O instance_types_en.nt.bz2 http://downloads.dbpedia.org/2015-04/core-i18n/en/instance-types-transitive_en.nt.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpltools import style\n",
    "import numpy as np\n",
    "style.use('ggplot')\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import shelve\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate n-gram corpus from annotated wikipedia texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    spark-submit --num-executors 20 --master yarn-client ./wikipedia/spark_anchors.py \"/data/wikipedia2015_plaintext_annotated\" \"/user/roman/wikipedia_anchors_orig\"\n",
    "    spark-submit --num-executors 20 --master yarn-client ./wikipedia/spark_orig_ngram_counts.py \"/user/roman/wikipedia_anchors_orig\" \"/user/roman/orig_ngram_counts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organic (human) linkings\n",
    "Since we want to prove we link entity types better than original (human) annonators, we need to first extract original linkings for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis: raw counts\n",
    "\n",
    "First we try to see what happens if we perform stupid linking by means of exact string matching, using canonical and \"redirect\" labels of wikipedia pages. All such labels uniquely link to a single entity (at least inside Wikipedia).\n",
    "\n",
    "We aggregate both the counts of the labels and of their lowercase versions for further analysis.\n",
    "Initially we will also generate DBPedia entity dictionary which maps labels to entity types (used later):\n",
    "\n",
    "    python dbpedia_dbm.py\n",
    "    spark-submit --executor-memory 5g --num-executors 20 --master yarn-client ./wikipedia/spark_plain_ngrams.py \"/data/wikipedia2015_plaintext_annotated\" \"/user/roman/wikipedia_ngrams\"\n",
    "    spark-submit --num-executors 20 --executor-memory 5g --master yarn-client ./wikipedia/spark_lowercase.py \"/user/roman/wikipedia_ngrams\" \"/user/roman/ngram_counts\"\n",
    "\n",
    "Retrieve label counts:\n",
    "\n",
    "    hdfs dfs -cat /user/roman/ngram_counts/* > dbpedia_counts_inferred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct original counts file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    spark-submit --master yarn-client --num-executors 20 ./wikipedia/spark_anchors.py \"/data/wikipedia_anchors\" \"/user/roman/orig_ngram_counts\"\n",
    "    hdfs dfs -cat /user/roman/orig_ngram_counts/* > dbpedia_counts_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>infer_lower</th>\n",
       "      <th>infer_upper</th>\n",
       "      <th>label</th>\n",
       "      <th>len</th>\n",
       "      <th>organ_lower</th>\n",
       "      <th>organ_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 0</td>\n",
       "      <td>  4</td>\n",
       "      <td>                     Feijo</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 0</td>\n",
       "      <td>  9</td>\n",
       "      <td>           Atlético_Celaya</td>\n",
       "      <td> 2</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 2</td>\n",
       "      <td> 11</td>\n",
       "      <td>               The_Gigolos</td>\n",
       "      <td> 2</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 0</td>\n",
       "      <td> 13</td>\n",
       "      <td> Socialist_Peasants'_Party</td>\n",
       "      <td> 3</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 6</td>\n",
       "      <td>  0</td>\n",
       "      <td>    Unadorned_rock-wallaby</td>\n",
       "      <td> 2</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   infer_lower  infer_upper                      label  len  organ_lower  \\\n",
       "0            0            4                      Feijo    1            0   \n",
       "1            0            9            Atlético_Celaya    2            0   \n",
       "2            2           11                The_Gigolos    2            0   \n",
       "3            0           13  Socialist_Peasants'_Party    3            0   \n",
       "4            6            0     Unadorned_rock-wallaby    2            0   \n",
       "\n",
       "   organ_upper  \n",
       "0            0  \n",
       "1            1  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_dict = {}\n",
    "for line in open('dbpedia_counts_inferred'):\n",
    "    label, values = line.split('\\t')\n",
    "    upper_count, lower_count = values.split(',')\n",
    "    count_dict[label] = {'infer_upper': int(upper_count), 'infer_lower': int(lower_count), 'len': len(label.split('_')),\n",
    "                       'label': label, 'organ_upper': 0, 'organ_lower': 0}\n",
    "for line in open('dbpedia_counts_original'):\n",
    "    label, values = line.split('\\t')\n",
    "    if label in count_dict:\n",
    "        upper_count, lower_count = values.split(',')\n",
    "        count_dict[label].update({'organ_upper': int(upper_count), 'organ_lower': int(lower_count)})\n",
    "counts_df = pd.DataFrame(count_dict.values())\n",
    "del count_dict\n",
    "counts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of inferred types: 339\n",
      "Total number of original types: 338\n",
      "set(['SnookerWorldRanking'])\n"
     ]
    }
   ],
   "source": [
    "a = set(counts_df[(counts_df.infer_upper + counts_df.infer_lower > 0)]['type'])\n",
    "b = set(counts_df[(counts_df.organ_upper + counts_df.organ_lower > 0)]['type'])\n",
    "print 'Total number of inferred types:',  len(a)\n",
    "print 'Total number of original types:',  len(b)\n",
    "print a.difference(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate excludes by ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_title(label):\n",
    "    label = label.split('_')\n",
    "    if len(label) == 1:\n",
    "        return False\n",
    "    first_letters = [x[0] for x in label if x[0].isupper()]\n",
    "    if len(first_letters) == 1:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Write excludes file, create lowercase matchings file\n",
    "dbpediadb_lower = {}\n",
    "#excludes = open('../mapreduce/PARAM_plus_achors/dbpedia_uri_excludes.txt', 'w')\n",
    "excludes = open('dbpedia_uri_excludes.txt', 'w')\n",
    "for row in counts_df.iterrows():\n",
    "    row = row[1]\n",
    "    exclude = False\n",
    "    label = row['label']\n",
    "    \n",
    "    # skip uppercase\n",
    "    if label.isupper():\n",
    "        continue\n",
    "    # skip titlecased labels than are bigrams and higher\n",
    "    if is_title(label):\n",
    "        # add to lower db\n",
    "        if row['organ_lower'] > 0 and row['organ_upper'] == 0:\n",
    "            dbpediadb_lower[label.lower()] = label\n",
    "        continue\n",
    "    if row['organ_upper'] == 0:\n",
    "        if row['infer_upper'] == 0:\n",
    "            pass\n",
    "        else:\n",
    "            if row['infer_lower'] > 0:\n",
    "                new_ratio = row['infer_upper']/float(row['infer_lower'])\n",
    "                if new_ratio < 20:\n",
    "                    exclude = True\n",
    "    else:\n",
    "        new_ratio = row['infer_upper']/float(row['infer_lower'] or 1)\n",
    "        orig_ratio = row['organ_upper']/float(row['organ_lower'] or 1)\n",
    "        if new_ratio < orig_ratio:\n",
    "            exclude = True\n",
    "    if exclude:\n",
    "        excludes.write(label+'\\n')\n",
    "    elif row['organ_lower'] > 0:\n",
    "        dbpediadb_lower[label.lower()] = label\n",
    "excludes.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#out = open('../mapreduce/PARAM_plus_achors/dbpediadb_lower.txt', 'w')\n",
    "out = open('dbpedia_lower_includes.txt', 'w')\n",
    "for lower_label, label in dbpediadb_lower.items():\n",
    "    out.write('%s\\t%s\\n' % (lower_label, label))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical significance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-test custom vs orig: (664.539616654572, 0.0)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "with open('/home/roman/berkeleylm/temp.log') as a:\n",
    "    simple_data = [float(x.split(';')[0]) for x in a]\n",
    "with open('/home/roman/berkeleylm/temp1.log') as a:\n",
    "    generic_data = [float(x.split(';')[0]) for x in a]\n",
    "del generic_data[-1]\n",
    "del simple_data[-1]\n",
    "\n",
    "#print 'T-test merged vs orig:', ttest_rel(merged_wiki_data, orig_wiki_data)\n",
    "#print 'T-test merged vs custom:', ttest_rel(merged_wiki_data, custom_wiki_data)\n",
    "print 'T-test custom vs orig:', ttest_rel(generic_data, simple_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>infer_lower</th>\n",
       "      <th>infer_upper</th>\n",
       "      <th>label</th>\n",
       "      <th>len</th>\n",
       "      <th>organ_lower</th>\n",
       "      <th>organ_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>265626</th>\n",
       "      <td> 0</td>\n",
       "      <td> 382</td>\n",
       "      <td> Saban</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        infer_lower  infer_upper  label  len  organ_lower  organ_upper\n",
       "265626            0          382  Saban    1            0            0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df[(counts_df.label == 'Saban')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotating text and computing precision/recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from kilogram.dataset.dbpedia import NgramEntityResolver\n",
    "import re\n",
    "ner = NgramEntityResolver(\"/home/roman/dbpedia/dbpedia_types.txt\", \"/home/roman/dbpedia/dbpedia_uri_excludes.txt\", \"/home/roman/dbpedia/dbpedia_lower_includes.txt\", \"/home/roman/dbpedia/dbpedia_redirects.txt\", \"/home/roman/dbpedia/dbpedia_2015-04.owl\")\n",
    "ENTITY_MATCH_RE = re.compile(r'<(.+?)\\|(.+?)>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/roman/language_models/wekex_test\n",
      "0.921052631579\n",
      "0.645161290323\n",
      "/home/roman/language_models/msnbc_test\n",
      "0.905213270142\n",
      "0.628289473684\n"
     ]
    }
   ],
   "source": [
    "test_data = ['/home/roman/language_models/wekex_test', '/home/roman/language_models/msnbc_test']\n",
    "precision = 0\n",
    "for filename in test_data:\n",
    "    print filename\n",
    "    recall = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    test_file = open(filename).read().splitlines()\n",
    "    for line in test_file:\n",
    "        entities = ENTITY_MATCH_RE.findall(line)\n",
    "        for entity in entities:\n",
    "            uri, text = entity\n",
    "            text = text.replace('_', ' ').replace(\"'s\", \"\")\n",
    "            uri = ner.redirects_file.get(uri, uri)\n",
    "            if uri in ner.dbpedia_types:\n",
    "                entities = ner.resolve_entities(text.split())\n",
    "                not_valid = [x for x in entities if x.startswith('<dbpedia:') and uri not in x]\n",
    "                valid = [x for x in entities if x.startswith('<dbpedia:') and uri in x]\n",
    "                fp += len(not_valid)\n",
    "                tp += len(valid)\n",
    "                if not valid:\n",
    "                    fn += 1\n",
    "    print tp/(tp+fp)\n",
    "    print tp/(tp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
