{
 "metadata": {
  "name": "",
  "signature": "sha256:a534d7436a850f682adf29503f304e3af87f002f47ba45f7cb8f64f1a522aecf"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small><i>This notebook was put together by [Roman Prokofyev](http://prokofyev.ch)@[eXascale Infolab](http://exascale.info/). Source and license info is on [GitHub](https://github.com/dragoon/kilogram/).</i></small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<small>\n",
      "This notebook is a part of bigger tutorial on fixing grammatical edits.\n",
      "\n",
      "* [Part 1: Extracting edits from StackExchange Data](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/notebooks/stackexchange_edit_extraction.ipynb)\n",
      "* **[Part 2: Processing Google Book N-grams dataset](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/notebooks/process_google_ngrams.ipynb)**\n",
      "* [Part 3: Computing association measures between words](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/notebooks/pmi_association_measures.ipynb)\n",
      "* [Part 4: Generic data analysis](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/notebooks/data_analysis_generic.ipynb)\n",
      "* [Part 5: Machine learning to fix grammar](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/notebooks/ml_grammar.ipynb)\n",
      "</small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Pre-processed dataset\n",
      "\n",
      "If you don't want to process data yourself, we have prepared the dataset processed according to the steps below in S3: http://coming.soon\n",
      "\n",
      "You just need to load this dataset into your local database to compute association measures (Step 2)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Prerequisites\n",
      "\n",
      "* Hadoop cluster with **Cloudera Hadoop Distribution** (CDH5): HDFS, YARN, PIG, HBASE"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 0: Downloading n-gram counts\n",
      "\n",
      "We need to download n-gram counts from some sufficiently large corpus.\n",
      "In this example we used **Google Books N-gram** corpus version **20120701**.\n",
      "The dataset is split into multiple parts and can be downloaded from here: http://storage.googleapis.com/books/ngrams/books/datasetsv2.html\n",
      "\n",
      "The tutorial assumes you alredy have the data aggregated by year."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 1: Aggregating counts and identifying entities\n",
      "Tte dataset contains n-gram counts per year, which we don't need here.\n",
      "Therefore we aggregate by year in **Hadoop** first.\n",
      "We also remove POS-tagged n-grams and lowercase everything.\n",
      "This step significantly reduces the amount of data we need to store, so we can, for example,\n",
      "load all **2grams** into **HBase**.\n",
      "\n",
      "The following table shows the sizes of ngrams at different processing stages:\n",
      "\n",
      "| Type   | Original Size (gzipped) | Year-aggregated | Final size |\n",
      "| ------ |:-----------------------:| :--------:| :--------: |\n",
      "| 1grams | 5.3GB | 342MB   | 90MB   |\n",
      "| 2grams | 135GB | 13.4GB  | 1.9GB  |\n",
      "| 3grams | 1.3TB | 190.6GB | 9.5GB  |\n",
      "| 4grams | 294GB | 50.9GB  | 17.6GB |\n",
      "| 5grams | XXXGB | 51.1GB  | 12GB   |\n",
      "\n",
      "\n",
      "At the final processing step, we try to aggregate n-grams containing various entities by their conceptual type using regular expressions and dictionaries.\n",
      "For example, we distinguish **numeric entities**, **person names**, **cities (geo-entities)** and **DBPedia entities** (see below).\n",
      "We will show how to use this information in the next part of this tutotial.\n",
      "\n",
      "Internally **numeric entities** have 5 types which are identified by *regular expressions*:\n",
      "* Datetimes (2 different types)\n",
      "* Percentages\n",
      "* Area or Volume metrics\n",
      "* Generic numbers\n",
      "\n",
      "**Person names** are identified by a dictionary lookup in the **names** corpus of the **NLTK** library.\n",
      "For each match we generate new n-gram with original string replaced by the \"PERSON\" token.\n",
      "\n",
      "**Cities** are also identified by a dictionary lookup. We use **Geonames 15000** as our dictionary. Since it is very small (~2Mb), we ship it with the **kilogram** library. For each match we generate new n-gram with original string replaced by the \"CITY\" token.\n",
      "\n",
      "**DBPedia** entities are a bit more complicated. On one side, there are many interesting properties inside DBPedia that we can leverage on. On the other side, it's **generally impossible to match** arbitrary DBPedia entity inside an n-gram due to lack of context. It would be only possible if we had a list of unambiguous entities that always match their string representations (but we don't have it).\n",
      "However, we found a way to remove incorrect entity linkings (i.e., entities that often match generic phrases) by applying statistical methods of outlying counts detection for DBPedia types and removing the outliers.\n",
      "\n",
      "*****"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That was the teoretical part. Coming to the n-gram processing itself.\n",
      "\n",
      "First clone the **[kilogram](https://github.com/dragoon/kilogram)** library:\n",
      "\n",
      "    git clone https://github.com/dragoon/kilogram.git\n",
      "    cd kilogram/mapreduce\n",
      "\n",
      "### Pre-filter n-grams\n",
      "Run the pre-filter job (again, on already year-aggregated data). The job removes POS tags, multiple punctuation symbols and some other stuff:\n",
      "\n",
      "    ./run_job.py [--reducers NUMBER_OF_REDUCERS] $RAW_NGRAMS_DIR $NGRAM_PREFILTER_DIR\n",
      "\n",
      "While the job is running, install **NLTK**, **kilogram** library and download the **names dataset** on every machine using *parallel-ssh*:\n",
      "\n",
      "    parallel-ssh -h hosts.txt -l root 'apt-get install -y python-dev libxml2-dev libxslt-dev'\n",
      "    parallel-ssh -h hosts.txt -l root 'pip install -U https://github.com/dragoon/kilogram/zipball/master'\n",
      "    parallel-ssh -h hosts.txt -l root 'python -m nltk.downloader -d /usr/local/lib/nltk_data names'\n",
      "*Hosts.txt* file should contain a list of hosts in your cluster, one per line.\n",
      "\n",
      "Pre-filtered n-grams are required to extract DBPedia entities. This method is covered in a separate tutorial:\n",
      "[Linking DBPedia Entities in N-gram corpus](http://nbviewer.ipython.org/github/dragoon/kilogram/blob/master/notebooks/dbpedia_ner.ipynb)\n",
      "\n",
      "### Post-filter n-grams\n",
      "After DBPedia entities are linked, run the post-filter job to resolve numeric entities and lowercase all other n-grams:\n",
      "\n",
      "    ./run_job.py -m ./filter/mapper_postfilter.py $NGRAM_PREFILTER_DIR:$ENTITY_DIR:$DBPEDIA_TYPE_DIR $NGRAM_POSTFILTER_DIR\n",
      "\n",
      "After post-filtering, n-grams should consume ~44GB.\n",
      "\n",
      "In the next step, we will extract only the n-grams we are interested in."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 1: Filtering preposition n-gram counts\n",
      "\n",
      "To efficiently compute association measures between words, we need efficient access to every n-gram count up to 3grams.\n",
      "Unfortunately, the new n-gram count sizes are still to large to be put in any **local key-value store**.\n",
      "\n",
      "Another issue with storing n-grams counts as simple key-values is high inefficiency for our **grammatical correction task**.\n",
      "For instance, to correct a preposition in a sentence, we will need to retrieve n-gram counts for all possible prepositions in consideration.\n",
      "\n",
      "Using simple key-value counts, we will have to make one request per preposition per n-gram.\n",
      "However, we know in advance which prepositions we want to consider, so we can aggregate all preposition counts into one value and use special n-gram as a key.\n",
      "We will call such n-grams **preposition n-grams**.\n",
      "\n",
      "Assuming that our current directory is something like ``~/kilogram/mapreduce``, we can filter preposition n-grams using the following scripts:\n",
      "\n",
      "    ./run_job.py -n 2 -m ./filter/mapper_filter.py --filter-file ../extra/preps.txt $NGRAM_POSTFILTER_DIR $2GRAMS_PREPS_OUT\n",
      "    ./run_job.py -n 3 -m ./filter/mapper_filter.py --filter-file ../extra/preps.txt $NGRAM_POSTFILTER_DIR $3GRAMS_PREPS_OUT \n",
      "    \n",
      "This way we will filter 2grams and 3grams that contain prepositions, and their sizes should be approximately 170MB and 2.9GB respectively. Now we can continue and load them to a local database.\n",
      "\n",
      "To compute association measures for 3grams, we would also require access to arbitrary 2gram and 1gram counts. We will show how to extract and load them to a storage in the next section."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 2: Loading n-gram counts into database\n",
      "\n",
      "Given what we descussed in the previous section, we decided to use the following solution:\n",
      "\n",
      "* **MongoDB** to store **preposition n-grams**, since they are small enough to fit into one machine;\n",
      "* **HBase** to store other arbitrary n-grams.\n",
      "\n",
      "To store **preposition n-grams** into MongoDB in the right format, we use scripts from the kilogram library. It needs to be installed via pip as well to run the scripts:\n",
      "\n",
      "    hdfs dfs -cat $2GRAMS_PREPS_OUT/* > preps_2grams\n",
      "cat preps_2grams | convert_to_mongo.py --sub preps.txt | sort -k1,1 -t $'\\t' > preps_2grams_mongo\n",
      "    \n",
      "    hdfs dfs -cat $3GRAMS_PREPS_OUT/* > preps_3grams\n",
      "cat preps_3grams | convert_to_mongo.py --sub preps.txt | sort -k1,1 -t $'\\t' > preps_3grams_mongo\n",
      "\n",
      "    insert_to_mongo.py -d ngrams --subs preps_3grams_mongo\n",
      "    insert_to_mongo.py -d ngrams --subs preps_2grams_mongo\n",
      "    \n",
      "File ``preps.txt`` represents our preposition set, which counts 49 prepositions in total.\n",
      "\n",
      "Compute and store **all 1grams** into mongo:\n",
      "\n",
      "    ./run_job.py -n 1 -m ./filter/mapper_filter.py $NGRAM_POSTFILTER_DIR $1GRAMS_OUT_DIR\n",
      "    hdfs dfs -cat $1GRAMS_OUT_DIR/* > all_1grams\n",
      "    insert_to_mongo.py -d 1grams all_1grams\n",
      "\n",
      "Next, we upload 2grams to **HBase** table names **ngrams2** using [Pig](http://pig.apache.org/): \n",
      "    \n",
      "    hbase shell\n",
      "    > create 'ngrams', 'ngram'\n",
      "    > ^D\n",
      "    pig -p table=ngrams -p path=$NGRAM_POSTFILTER_DIR -p n=2 ../extra/hbase_upload.pig\n",
      "\n",
      "**Grab another cup of something, this will take a while.**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Step 3: Determiner skips"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Filter arbitrary n-grams with determiner skips and insert them to HBase:\n",
      "\n",
      "    /run_job.py -m ./grammar/mapper_DT_strip.py $NGRAM_POSTFILTER_DIR $NGRAM_DT_SKIPS_DIR\n",
      "    pig -p table=ngrams2 -p path=$NGRAM_DT_SKIPS_DIR -p n=2 ../extra/hbase_upload.pig\n",
      "pig -p table=ngrams2 -p path=$NGRAM_DT_SKIPS_DIR -p n=3 ../extra/hbase_upload.pig\n",
      "Filter **preposition n-grams** with determiner skips and insert them to MongoDB:\n",
      "    \n",
      "    ./run_job.py -m ./grammar/mapper_DT_strip.py --filter-file ../extra/preps.txt $NGRAM_POSTFILTER_DIR $NGRAM_DT_SKIPS_DIR\n",
      "    hdfs dfs -cat $NGRAM_DT_SKIPS_DIR/* > preps_dt_skips\n",
      "cat preps_dt_skips | convert_to_mongo.py --sub preps.txt | sort -k1,1 -t $'\\t' > preps_dt_skips_mongo\n",
      "    insert_to_mongo.py -d ngrams --subs preps_dt_skips_mongo\n",
      "    \n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next part of this notebook describes how to calculate association measures between words in order to fix grammar."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}